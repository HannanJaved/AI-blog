<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The ML Crash Course Blog</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <!-- Chosen Palette: Calm Neutrals -->
    <!-- Application Structure Plan: The application is now a blog-style website. A main content area displays articles, while a sidebar provides navigation via "Recent Posts" and highlights a "Featured Post." This structure is more engaging and familiar for readers, allowing them to focus on one topic at a time. Each module is a separate "article" div, and clicking a link in the sidebar smoothly scrolls to that article. This is more user-friendly for a content-heavy site than a single, long scrolling page. -->
    <!-- Visualization & Content Choices: 
        - All previous visualizations and interactive demos are retained but are now embedded within the blog post format.
        - The goal is to present each module as a self-contained article that is easy to read and understand, with visuals supporting the text.
        - The interactive elements for k-NN and K-Means are particularly effective in a blog format as they break up long sections of text and allow for hands-on learning.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f5f5f4; /* stone-100 */
            color: #292524; /* stone-800 */
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 350px;
            max-height: 400px;
        }
        .static-chart-container {
             position: relative;
            width: 100%;
            max-width: 500px;
            margin-left: auto;
            margin-right: auto;
            height: 300px;
            max-height: 350px;
        }
        .formula {
            font-family: 'Times New Roman', serif;
            background-color: #e7e5e4; /* stone-200 */
            padding: 1rem;
            border-radius: 0.5rem;
            text-align: center;
            overflow-x: auto;
            font-size: 1.1rem;
            margin: 1rem 0;
            color: #44403c; /* stone-700 */
        }
        .decision-node {
            border: 2px solid #5eead4; /* teal-300 */
            background-color: #f0fdfa; /* teal-50 */
        }
        .leaf-node {
            border: 2px solid #67e8f9; /* cyan-300 */
            background-color: #ecfeff; /* cyan-50 */
        }
        .tree-container ul {
            position: relative;
            padding-left: 20px;
            list-style: none;
        }
        .tree-container li {
            position: relative;
            padding-top: 10px;
        }
        .tree-container li::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            border-left: 2px solid #9ca3af;
            width: 20px;
            height: 100%;
        }
        .tree-container li:last-child::before {
            height: 28px;
        }
        .tree-container li > div::before {
            content: '';
            position: absolute;
            top: 28px;
            left: -20px;
            border-top: 2px solid #9ca3af;
            width: 20px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 400px;
            }
        }
    </style>
</head>
<body class="antialiased">

    <!-- Header -->
    <header class="bg-white shadow-md">
        <div class="max-w-7xl mx-auto py-6 px-4 sm:px-6 lg:px-8">
            <h1 class="text-3xl font-bold text-teal-700">The ML Crash Course Blog</h1>
            <p class="mt-1 text-stone-600">A detailed and interactive guide to the fundamentals of Machine Learning.</p>
        </div>
    </header>

    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8">
        <div class="lg:flex lg:space-x-8">
            <!-- Main Content -->
            <main class="flex-1">
                <div class="space-y-16">
                    <!-- Article 1 -->
                    <article id="module1" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-teal-700 border-b-2 border-teal-200 pb-2 mb-6">Module 1: Introduction to Machine Learning</h2>
                        <div class="space-y-8 text-lg text-stone-700">
                            <p>This module provides a high-level overview of machine learning, establishing the core goal of generalization and introducing the main paradigms of learning. It sets the stage for understanding not just what ML is, but why it's structured the way it is.</p>
                            
                            <div>
                                <h3 class="text-2xl font-semibold mt-4 text-stone-800">1.1. What is Machine Learning?</h3>
                                <p class="mt-2">At its core, Machine Learning is the science of getting computers to act without being explicitly programmed. Instead of writing a fixed set of rules, we provide algorithms with large amounts of data and let them learn the rules and patterns for themselves. The formal definition by Tom Mitchell provides a more structured view:</p>
                                <blockquote class="mt-4 p-4 bg-stone-100 border-l-4 border-teal-500">
                                    "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."
                                </blockquote>
                                <p class="mt-4">Let's break this down with a real-world example: a spam filter.</p>
                                <ul class="list-disc list-inside mt-2 space-y-1 text-base">
                                    <li><strong>Task (T):</strong> To classify emails as either "spam" or "not spam".</li>
                                    <li><strong>Experience (E):</strong> A massive dataset of emails, where each one has already been labeled by humans as spam or not spam.</li>
                                    <li><strong>Performance (P):</strong> The percentage of emails the program correctly classifies.</li>
                                </ul>
                                <p class="mt-2">The ultimate goal is **Generalization**: the ability of the model to perform accurately on new, unseen emails it has never encountered before, not just the ones it was trained on.</p>
                            </div>

                            <div>
                                <h3 class="text-2xl font-semibold mt-4 text-stone-800">1.2. The Pillars of ML</h3>
                                <div class="space-y-6 mt-4">
                                    <div>
                                        <h4 class="font-semibold text-teal-600">Supervised Learning</h4>
                                        <p class="text-base text-stone-600 mb-2">Think of this as learning with a teacher or an answer key. The model is given a dataset where each data point is tagged with a correct label or output. The goal is to learn a mapping function that can predict the output for new, unseen data. It's like a student studying flashcards where one side is the question (input) and the other is the answer (label).</p>
                                        <div class="border p-4 rounded-lg mt-2 text-center">
                                            <p class="font-mono text-sm">[Email Content, Label] ‚Üí Model ‚Üí Prediction</p>
                                            <div class="flex justify-center items-center space-x-4 mt-2">
                                                <div class="text-center"><p class="text-4xl">‚úâÔ∏è</p><p class="text-xs font-bold text-red-600">SPAM</p></div>
                                                <div class="text-4xl font-thin">‚Üí</div>
                                                <div class="p-2 border-2 border-dashed rounded-md">ü§ñ</div>
                                                <div class="text-4xl font-thin">‚Üí</div>
                                                <div><p class="text-4xl">üìß</p><p class="font-bold text-teal-600">NOT SPAM</p></div>
                                            </div>
                                        </div>
                                    </div>
                                    <div>
                                        <h4 class="font-semibold text-teal-600">Unsupervised Learning</h4>
                                        <p class="text-base text-stone-600 mb-2">This is learning without an answer key. The model is given data without explicit labels and must find structure or patterns on its own. A common task is clustering, or grouping similar data points together. Imagine being given a box of mixed Lego bricks and asked to sort them into piles based on their shape and color without any instructions.</p>
                                        <div class="border p-4 rounded-lg mt-2 text-center">
                                            <p class="font-mono text-sm">[Unlabeled Data] ‚Üí Model ‚Üí [Clusters/Patterns]</p>
                                            <div class="flex justify-center items-center space-x-4 mt-2">
                                                <div class="text-4xl">üçéüçåüçè</div>
                                                <div class="text-4xl font-thin">‚Üí</div>
                                                <div class="p-2 border-2 border-dashed rounded-md">ü§ñ</div>
                                                <div class="text-4xl font-thin">‚Üí</div>
                                                <div><p class="text-4xl">üçéüçè | üçå</p><p class="text-xs font-bold text-teal-600">2 GROUPS</p></div>
                                            </div>
                                        </div>
                                    </div>
                                    <div>
                                        <h4 class="font-semibold text-teal-600">Reinforcement Learning</h4>
                                        <p class="text-base text-stone-600 mb-2">This is learning through trial and error, like training a pet. An "agent" (the model) learns to make decisions by performing actions in an "environment" to maximize a cumulative reward. It learns from the consequences of its actions, receiving positive rewards for good actions and penalties for bad ones.</p>
                                        <div class="border p-4 rounded-lg mt-2 text-center">
                                            <p class="font-mono text-sm">Agent ‚Üî Environment ‚Üí [Action, State, Reward]</p>
                                            <div class="flex justify-center items-center space-x-2 mt-2">
                                                <div class="text-4xl">ü§ñ</div>
                                                <div class="text-center"><p class="text-sm">Action</p><p class="text-2xl">‚Üî</p><p class="text-sm">State, Reward</p></div>
                                                <div class="text-4xl">üåç</div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            
                            <div>
                                <h3 class="text-2xl font-semibold mt-8 text-stone-800">1.3. The ML Workflow</h3>
                                <p class="mt-2">Nearly every machine learning project follows a standard, iterative lifecycle. Understanding these steps is crucial for building effective models.</p>
                                <ol class="list-decimal list-inside mt-4 space-y-4 text-base">
                                    <li><strong>Problem Framing:</strong> Define the objective. What question are you trying to answer? Is it a regression problem (predicting a number) or a classification problem (predicting a category)? What metric will define success?</li>
                                    <li><strong>Data Collection & Preprocessing:</strong> Gather the necessary data. This is often the most time-consuming step, involving cleaning the data (handling missing values, outliers), feature engineering (creating new input signals), and scaling features to a common range.</li>
                                    <li><strong>Model Selection & Training:</strong> Choose a suitable algorithm and feed it the prepared training data. The model learns the underlying patterns during this "training" phase.</li>
                                    <li><strong>Evaluation:</strong> Assess the model's performance on a held-out test set‚Äîdata it has never seen before. This gives an unbiased estimate of how it will perform in the real world.</li>
                                    <li><strong>Hyperparameter Tuning:</strong> Adjust the model's settings (hyperparameters, like the 'k' in k-NN) to improve performance, often using a validation dataset to prevent overfitting to the test set.</li>
                                    <li><strong>Deployment:</strong> Integrate the trained model into a production environment where it can make predictions on new, live data.</li>
                                </ol>
                            </div>
                        </div>
                    </article>

                    <!-- Article 2 -->
                    <article id="module2" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-teal-700 border-b-2 border-teal-200 pb-2 mb-6">Module 2: Supervised Learning - Regression</h2>
                         <p class="text-lg text-stone-700 mb-6">Regression models are used when the goal is to predict a continuous numerical value. This module focuses on Linear Regression, the foundational algorithm for this task.</p>
                        <h3 class="text-2xl font-semibold mb-2 text-stone-800">2.1. Linear Regression</h3>
                        <p>Fits a "line of best fit" to the data. It models a linear relationship between input features (X) and a continuous target variable (y). For example, predicting a house's price based on its square footage.</p>
                        <div class="formula">≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çöx‚Çö</div>
                         <h4 class="font-semibold mt-4 text-stone-700">How it Learns:</h4>
                        <p>The model learns the optimal coefficients (Œ≤) by minimizing a **loss function**, typically the **Mean Squared Error (MSE)**, which measures the average squared difference between predicted (≈∑) and actual (y) values. This minimization is usually performed using an optimization algorithm like **Gradient Descent**. Imagine the loss function as a hilly landscape; Gradient Descent is like taking small steps downhill from a random starting point until you reach the lowest valley, which represents the minimum error.</p>
                        <div class="static-chart-container mt-4">
                            <canvas id="linearRegressionChart"></canvas>
                        </div>
                        <h4 class="font-semibold mt-4 text-stone-700">Key Assumptions:</h4>
                        <ol class="list-decimal list-inside mt-2 space-y-2">
                            <li><strong>Linearity:</strong> The relationship between inputs and the output is linear. If the true relationship is curved (e.g., house price doesn't increase linearly with size forever), the model will be inherently inaccurate.</li>
                            <li><strong>Independence:</strong> The errors (residuals) are independent. This means the error of one prediction is not influenced by the error of another. (Commonly violated in time-series data, where yesterday's stock price error affects today's).</li>
                            <li><strong>Homoscedasticity:</strong> The errors have constant variance across all levels of the input features. A violation (heteroscedasticity) might mean the model is very accurate for cheap houses but highly inaccurate for expensive ones.</li>
                            <li><strong>Normality of Errors:</strong> The errors are normally distributed. This is particularly important for statistical inference on the coefficients (e.g., determining if a feature is statistically significant).</li>
                        </ol>
                    </article>

                    <!-- Article 3 -->
                    <article id="module3" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-teal-700 border-b-2 border-teal-200 pb-2 mb-6">Module 3: Supervised Learning - Classification</h2>
                         <p class="text-lg text-stone-700 mb-6">Classification models are used when the goal is to predict a discrete category or class label. This module covers the foundational algorithms for this task.</p>
                        
                        <div class="space-y-8">
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">3.1. Logistic Regression</h3>
                                <p>A baseline for classification that predicts the probability an input belongs to a class by passing a linear equation through a Sigmoid function. Example: Predicting if a loan application will be approved (Yes/No) based on income and credit score.</p>
                                <div class="formula">pÃÇ = 1 / (1 + e<sup>-(Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ...)</sup>)</div>
                                 <h4 class="font-semibold mt-4 text-stone-700">How it Learns:</h4>
                                <p>Unlike Linear Regression's MSE, Logistic Regression learns its coefficients by minimizing a different loss function called **Log Loss (or Binary Cross-Entropy)**. This function penalizes confident but incorrect predictions more heavily, making it suitable for probabilistic classification.</p>
                                 <div class="static-chart-container mt-4">
                                    <canvas id="logisticRegressionChart"></canvas>
                                </div>
                            </div>
                            
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">3.2. k-Nearest Neighbors (k-NN) Interactive Demo</h3>
                                <p class="mb-4 text-stone-700">A "lazy" algorithm that classifies a new point based on the majority vote of its 'k' closest neighbors. Example: Recommending a movie to a user based on the 'k' most similar users and what they liked.</p>
                                 <h4 class="font-semibold mt-4 text-stone-700">How it Learns:</h4>
                                <p>k-NN is a "lazy learner" because it doesn't have a distinct training phase. The "learning" consists of simply storing the entire training dataset in memory. All computation happens at prediction time, where it calculates the distance from a new point to every point in the training data to find its nearest neighbors.</p>
                                <div class="chart-container">
                                    <canvas id="knnChart"></canvas>
                                </div>
                                <div class="mt-4 flex items-center justify-center space-x-4">
                                    <label for="kSlider" class="font-medium text-stone-700">k = <span id="kValue">3</span></label>
                                    <input type="range" id="kSlider" min="1" max="15" value="3" step="2" class="w-64">
                                     <button id="resetKnnBtn" class="px-4 py-2 bg-teal-600 text-white rounded-lg hover:bg-teal-700 transition">Reset</button>
                                </div>
                            </div>

                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">3.3. Support Vector Machines (SVMs)</h3>
                                <p>A "maximum margin" classifier that finds the hyperplane that best separates the classes by maximizing the distance to the nearest points of any class (the support vectors).</p>
                                 <h4 class="font-semibold mt-4 text-stone-700">How it Learns:</h4>
                                <p>SVMs learn by solving a constrained optimization problem. The goal is to find the parameters (w, b) of the hyperplane that maximize the margin, subject to the constraint that all data points are correctly classified. This is a complex mathematical problem often solved using techniques like Quadratic Programming.</p>
                                <div class="mt-4 p-4 border border-stone-200 rounded-lg flex justify-center items-center">
                                    <div class="w-2 h-48 bg-stone-300 relative">
                                        <div class="absolute top-1/2 -translate-y-1/2 left-1/2 -translate-x-1/2 w-48 h-1 bg-teal-500"></div>
                                        <div class="absolute top-8 left-12 w-4 h-4 bg-blue-500 rounded-full"></div>
                                        <div class="absolute top-16 left-20 w-4 h-4 bg-blue-500 rounded-full border-2 border-teal-500"></div>
                                        <div class="absolute bottom-8 right-12 w-4 h-4 bg-red-500 rounded-full"></div>
                                        <div class="absolute bottom-16 right-20 w-4 h-4 bg-red-500 rounded-full border-2 border-teal-500"></div>
                                        <p class="absolute -top-2 left-1/2 -translate-x-1/2 text-sm text-stone-600">Margin</p>
                                    </div>
                                </div>
                            </div>

                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">3.4. Decision Trees</h3>
                                <p>A flowchart of "if-then-else" questions that splits the data into purer subgroups. Example: A bank using a decision tree to decide if a customer is eligible for a loan.</p>
                                 <h4 class="font-semibold mt-4 text-stone-700">How it Learns:</h4>
                                <p>The tree is built using a process called **recursive partitioning**. Starting with the entire dataset at the root node, the algorithm searches for the feature and threshold that create the "best" split‚Äîthe one that results in the greatest reduction in impurity (e.g., using Gini Impurity or Information Gain). This process is repeated for each new node until a stopping criterion (like maximum depth) is met.</p>
                                <div class="mt-4 p-4 border border-stone-200 rounded-lg tree-container">
                                    <ul>
                                        <li>
                                            <div class="p-2 decision-node rounded-md inline-block">Outlook = Sunny?</div>
                                            <ul>
                                                <li>
                                                    <div class="p-2 decision-node rounded-md inline-block">Humidity > 70?</div>
                                                    <ul>
                                                        <li><div class="p-2 leaf-node rounded-md inline-block">Don't Play</div></li>
                                                        <li><div class="p-2 leaf-node rounded-md inline-block">Play</div></li>
                                                    </ul>
                                                </li>
                                                <li>
                                                    <div class="p-2 leaf-node rounded-md inline-block">Play</div>
                                                </li>
                                            </ul>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </article>

                    <!-- Article 4 -->
                    <article id="module4" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-teal-700 border-b-2 border-teal-200 pb-2 mb-6">Module 4: Unsupervised Learning</h2>
                        <p class="text-lg text-stone-700 mb-6">Unsupervised learning deals with unlabeled data, seeking to find hidden patterns or intrinsic structures. This module explores the two primary tasks: clustering data into groups and reducing dimensionality to simplify data.</p>
                        
                         <div class="space-y-8">
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">4.1. K-Means Clustering Interactive Demo</h3>
                                <p class="mb-4 text-stone-700">A centroid-based algorithm that partitions data into 'k' clusters. Example: A marketing team using K-Means to segment customers into distinct groups for targeted campaigns.</p>
                                 <h4 class="font-semibold mt-4 text-stone-700">How it Learns:</h4>
                                <p>K-Means uses an iterative algorithm called **Expectation-Maximization (E-M)**. In the **E-step**, each data point is assigned to its nearest centroid. In the **M-step**, the centroids are recalculated as the mean of the points assigned to them. These two steps are repeated until the cluster assignments no longer change.</p>
                                <div class="chart-container">
                                    <canvas id="kmeansChart"></canvas>
                                </div>
                                <div class="mt-4 text-center">
                                    <button id="runKmeansBtn" class="px-4 py-2 bg-teal-600 text-white rounded-lg hover:bg-teal-700 transition">Run K-Means (k=3)</button>
                                </div>
                            </div>

                             <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">4.2. Principal Component Analysis (PCA)</h3>
                                <p class="mb-4 text-stone-700">A dimensionality reduction technique that projects data onto a smaller number of uncorrelated axes ("principal components") that capture the maximum variance.</p>
                                <ul class="space-y-2">
                                    <li><strong class="font-semibold text-teal-600">Covariance Matrix (Œ£):</strong> Describes the variance of each feature and the covariance between pairs of features.</li>
                                    <li><strong class="font-semibold text-teal-600">Eigenvectors & Eigenvalues:</strong> An eigenvector of the covariance matrix is a direction in the data. The corresponding eigenvalue indicates how much variance lies along that direction. PCA finds and ranks these to identify the principal components.</li>
                                </ul>
                            </div>
                        </div>
                    </article>

                    <!-- Article 5 -->
                    <article id="module5" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-teal-700 border-b-2 border-teal-200 pb-2 mb-6">Module 5: Model Training & Evaluation</h2>
                        <p class="text-lg text-stone-700 mb-6">Building a model is only half the battle; we must also rigorously evaluate its performance and understand its limitations. This module covers the crucial concepts of model validation, key performance metrics, and the fundamental Bias-Variance Tradeoff.</p>

                        <div class="space-y-8">
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">5.1. Key Metrics</h3>
                                <ul class="space-y-2">
                                    <li><strong>Precision:</strong> Of all positive predictions, how many were correct? <div class="formula text-base">TP / (TP + FP)</div></li>
                                    <li><strong>Recall:</strong> Of all actual positives, how many did the model find? <div class="formula text-base">TP / (TP + FN)</div></li>
                                    <li><strong>F1-Score:</strong> The harmonic mean of Precision and Recall. <div class="formula text-base">2 * (Precision * Recall) / (Precision + Recall)</div></li>
                                    <li><strong>ROC/AUC:</strong> AUC measures the ability of a model to distinguish between classes. 1.0 is perfect, 0.5 is random.</li>
                                </ul>
                            </div>

                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">5.2. The Bias-Variance Tradeoff</h3>
                                <p class="mb-4 text-stone-700">This is the central challenge in supervised learning. A simple model (low complexity) is "underfit" and has high bias (it's systematically wrong). A complex model (high complexity) is "overfit" and has high variance (it's too sensitive to the training data). The goal is to find the "sweet spot" of complexity that minimizes the total error.</p>
                                <div class="static-chart-container">
                                    <canvas id="biasVarianceChart"></canvas>
                                </div>
                            </div>
                            
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">5.3. Regularization</h3>
                                <p>Techniques to prevent overfitting by adding a penalty to the loss function. The shapes below represent the "constraint region" for the coefficients. The optimal solution is where the loss function contours first touch this region.</p>
                                <div class="grid md:grid-cols-2 gap-6 mt-4">
                                    <div>
                                        <h4 class="font-semibold text-teal-600 text-center">L1 (Lasso)</h4>
                                        <p class="text-sm text-center text-stone-600">The diamond shape makes it likely the contours will touch at an axis, forcing a coefficient to zero.</p>
                                        <div class="mt-2 p-4 border border-stone-200 rounded-lg h-32 flex items-center justify-center">
                                            <div class="w-24 h-24 border-2 border-dashed border-teal-400 transform rotate-45"></div>
                                            <div class="absolute w-24 h-12 bg-blue-200 border border-blue-400 rounded-md"></div>
                                        </div>
                                    </div>
                                    <div>
                                        <h4 class="font-semibold text-teal-600 text-center">L2 (Ridge)</h4>
                                        <p class="text-sm text-center text-stone-600">The circular shape makes it unlikely the contours will touch exactly at an axis, so coefficients get small but not zero.</p>
                                        <div class="mt-2 p-4 border border-stone-200 rounded-lg h-32 flex items-center justify-center">
                                            <div class="w-24 h-24 border-2 border-dashed border-teal-400 rounded-full"></div>
                                            <div class="absolute w-20 h-20 bg-blue-200 border border-blue-400 rounded-full"></div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </article>
                    
                    <!-- Article 6 -->
                    <article id="module6" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-teal-700 border-b-2 border-teal-200 pb-2 mb-6">Module 6: Advanced Topics & Deep Learning</h2>
                        <p class="text-lg text-stone-700 mb-6">This module introduces more advanced theoretical concepts and provides a glimpse into deep learning, the subfield of ML that powers many state-of-the-art applications.</p>
                        <div class="space-y-8">
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">6.1. Feed-Forward Neural Network</h3>
                                <p>A model inspired by the brain, composed of interconnected "neurons" organized in layers.</p>
                                <div class="flex justify-center items-center space-x-8 p-4 mt-4 border-t border-stone-200">
                                    <div class="flex flex-col items-center space-y-4"><p class="font-semibold">Input</p><div class="w-8 h-8 bg-blue-300 rounded-full"></div><div class="w-8 h-8 bg-blue-300 rounded-full"></div></div>
                                    <p class="text-4xl font-thin">‚Üí</p>
                                    <div class="flex flex-col items-center space-y-4"><p class="font-semibold">Hidden Layer</p><div class="w-8 h-8 bg-teal-300 rounded-full"></div><div class="w-8 h-8 bg-teal-300 rounded-full"></div><div class="w-8 h-8 bg-teal-300 rounded-full"></div></div>
                                    <p class="text-4xl font-thin">‚Üí</p>
                                    <div class="flex flex-col items-center space-y-4"><p class="font-semibold">Output</p><div class="w-8 h-8 bg-red-300 rounded-full"></div></div>
                                </div>
                            </div>
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">6.2. How Neural Networks Learn</h3>
                                <p>Networks learn by minimizing a loss function. This is achieved through an optimization process involving two key algorithms:</p>
                                <ul class="list-disc list-inside mt-2 space-y-2">
                                    <li><strong>Gradient Descent:</strong> An optimization algorithm that iteratively adjusts the model's parameters (weights and biases) to find the minimum of the loss function. The size of the adjustments is controlled by the **Learning Rate (Œ±)**.</li>
                                    <li><strong>Backpropagation:</strong> The algorithm used to efficiently calculate the gradient (the direction of steepest ascent) of the loss function with respect to every weight and bias in the network. It works by propagating the error backward from the output layer to the input layer.</li>
                                </ul>
                                 <div class="mt-4 p-4 border border-stone-200 rounded-lg text-center">
                                    <p class="font-semibold">1. Forward Pass ‚Üí</p>
                                    <p class="text-sm text-stone-600">Input data flows through the network to produce an output (prediction).</p>
                                    <p class="text-2xl my-2">‚Üì</p>
                                    <p class="font-semibold">2. Compute Loss</p>
                                    <p class="text-sm text-stone-600">Compare the prediction to the true label to calculate the error.</p>
                                    <p class="text-2xl my-2">‚Üì</p>
                                    <p class="font-semibold">3. Backward Pass (Backpropagation) ‚Üê</p>
                                    <p class="text-sm text-stone-600">Calculate the gradient of the loss with respect to each parameter.</p>
                                    <p class="text-2xl my-2">‚Üì</p>
                                    <p class="font-semibold">4. Update Parameters</p>
                                    <p class="text-sm text-stone-600">Adjust weights and biases using Gradient Descent to reduce the loss.</p>
                                </div>
                            </div>
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">6.3. Foundational Concepts - Bayes' Theorem</h3>
                                <p class="mb-4">The theorem provides a way to formally update a belief or hypothesis in light of new evidence. It's the mathematical rule for learning from experience.</p>
                                <div class="formula">P(A|B) = (P(B|A) * P(A)) / P(B)</div>
                                <ul class="space-y-4 mt-4">
                                    <li><strong class="font-semibold text-teal-600">P(A|B) - Posterior:</strong> Your updated belief about hypothesis A after seeing evidence B.</li>
                                    <li><strong class="font-semibold text-teal-600">P(B|A) - Likelihood:</strong> The probability of observing evidence B if hypothesis A is true.</li>
                                    <li><strong class="font-semibold text-teal-600">P(A) - Prior:</strong> Your initial belief about hypothesis A before seeing any evidence.</li>
                                    <li><strong class="font-semibold text-teal-600">P(B) - Evidence:</strong> The total probability of observing the evidence B.</li>
                                </ul>
                            </div>
                             <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">6.4. Generative vs. Discriminative Models</h3>
                                <div class="grid md:grid-cols-2 gap-6">
                                    <div>
                                        <h4 class="font-semibold text-teal-600">Discriminative Models</h4>
                                        <p>Learn the decision boundary between classes. They model P(Y|X).</p>
                                        <p class="text-sm text-stone-500">Examples: Logistic Regression, SVMs</p>
                                    </div>
                                    <div>
                                        <h4 class="font-semibold text-teal-600">Generative Models</h4>
                                        <p>Learn the underlying distribution of the data. They model P(X, Y).</p>
                                        <p class="text-sm text-stone-500">Examples: Naive Bayes, GMMs</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </article>
                </div>
            </main>

            <!-- Sidebar -->
            <aside class="w-80 hidden lg:block">
                <div class="sticky top-8 space-y-8">
                    <div class="bg-white p-6 rounded-lg shadow-sm">
                        <h3 class="text-xl font-semibold text-stone-800 mb-4">Featured Post</h3>
                        <a href="#module5" class="block group">
                            <div class="h-32 bg-teal-500 rounded-lg mb-4 flex items-center justify-center">
                               <canvas id="biasVarianceChartSmall"></canvas>
                            </div>
                            <h4 class="font-bold text-teal-700 group-hover:underline">The Bias-Variance Tradeoff</h4>
                            <p class="text-sm text-stone-600">Understanding this core concept is key to preventing overfitting and building robust models.</p>
                        </a>
                    </div>
                    <div class="bg-white p-6 rounded-lg shadow-sm">
                        <h3 class="text-xl font-semibold text-stone-800 mb-4">Recent Posts</h3>
                        <nav class="space-y-3">
                            <a href="#module1" class="block font-medium text-stone-700 hover:text-teal-600">1. Introduction to ML</a>
                            <a href="#module2" class="block font-medium text-stone-700 hover:text-teal-600">2. Supervised: Regression</a>
                            <a href="#module3" class="block font-medium text-stone-700 hover:text-teal-600">3. Supervised: Classification</a>
                            <a href="#module4" class="block font-medium text-stone-700 hover:text-teal-600">4. Unsupervised Learning</a>
                            <a href="#module5" class="block font-medium text-stone-700 hover:text-teal-600">5. Training & Evaluation</a>
                            <a href="#module6" class="block font-medium text-stone-700 hover:text-teal-600">6. Advanced Topics & DL</a>
                        </nav>
                    </div>
                </div>
            </aside>
        </div>
    </div>

<script>
document.addEventListener('DOMContentLoaded', () => {

    // Navigation scroll highlighting
    const sections = document.querySelectorAll('article');
    const navLinks = document.querySelectorAll('#desktop-nav a');

    const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
            if (entry.isIntersecting) {
                navLinks.forEach(link => {
                    link.classList.toggle('active', link.getAttribute('href').substring(1) === entry.target.id);
                });
            }
        });
    }, { rootMargin: '-20% 0px -80% 0px', threshold: 0 });

    sections.forEach(section => observer.observe(section));

    // Static Linear Regression Chart
    new Chart(document.getElementById('linearRegressionChart'), {
        type: 'scatter',
        data: {
            datasets: [{
                label: 'Data Points',
                data: [{x:1, y:2}, {x:2, y:2.5}, {x:3, y:4}, {x:4, y:4.5}, {x:5, y:6}, {x:6, y:5.5}],
                backgroundColor: 'rgba(59, 130, 246, 0.5)',
            }, {
                label: 'Line of Best Fit',
                data: [{x:0, y:1}, {x:7, y:7}],
                type: 'line',
                borderColor: 'rgba(239, 68, 68, 1)',
                borderWidth: 2,
                fill: false,
                pointRadius: 0,
            }]
        },
        options: { responsive: true, maintainAspectRatio: false, scales: { x: { min: 0, max: 7 }, y: { min: 0, max: 8 } }, plugins: { legend: { display: false } } }
    });

    // Static Logistic Regression Chart
    new Chart(document.getElementById('logisticRegressionChart'), {
        type: 'scatter',
        data: {
            datasets: [{
                label: 'Class 0',
                data: [{x:1, y:0}, {x:2, y:0}, {x:3, y:0}, {x:4, y:0}],
                backgroundColor: 'rgba(59, 130, 246, 0.5)',
                pointRadius: 6
            }, {
                label: 'Class 1',
                data: [{x:6, y:1}, {x:7, y:1}, {x:8, y:1}, {x:9, y:1}],
                backgroundColor: 'rgba(239, 68, 68, 0.5)',
                pointRadius: 6
            }, {
                label: 'Sigmoid Function',
                data: Array.from({length: 100}, (_, i) => {
                    const x = i * 0.1;
                    const y = 1 / (1 + Math.exp(-(x - 5)));
                    return {x, y};
                }),
                type: 'line',
                borderColor: 'rgba(16, 185, 129, 1)',
                borderWidth: 2,
                fill: false,
                pointRadius: 0,
                tension: 0.1
            }]
        },
        options: { responsive: true, maintainAspectRatio: false, scales: { x: { min: 0, max: 10 }, y: { min: -0.1, max: 1.1 } }, plugins: { legend: { display: false } } }
    });


    // k-NN Chart
    let knnChart;
    const knnCanvas = document.getElementById('knnChart');
    const kSlider = document.getElementById('kSlider');
    const kValueSpan = document.getElementById('kValue');
    const resetKnnBtn = document.getElementById('resetKnnBtn');
    let knnPoints = [];
    let newPoint = null;

    function generateKnnData() {
        knnPoints = [];
        for (let i = 0; i < 20; i++) {
            knnPoints.push({ x: Math.random() * 5, y: Math.random() * 5, label: 0 });
            knnPoints.push({ x: Math.random() * 5 + 5, y: Math.random() * 5 + 5, label: 1 });
        }
        newPoint = null;
    }

    function drawKnnChart() {
        const k = parseInt(kSlider.value);
        kValueSpan.textContent = k;

        const datasets = [{
            label: 'Class 0',
            data: knnPoints.filter(p => p.label === 0),
            backgroundColor: 'rgba(59, 130, 246, 0.5)',
            borderColor: 'rgba(59, 130, 246, 1)',
            pointRadius: 6,
        }, {
            label: 'Class 1',
            data: knnPoints.filter(p => p.label === 1),
            backgroundColor: 'rgba(239, 68, 68, 0.5)',
            borderColor: 'rgba(239, 68, 68, 1)',
            pointRadius: 6,
        }];

        if (newPoint) {
            const distances = knnPoints.map(p => ({
                dist: Math.sqrt((p.x - newPoint.x) ** 2 + (p.y - newPoint.y) ** 2),
                label: p.label
            }));
            distances.sort((a, b) => a.dist - b.dist);
            const neighbors = distances.slice(0, k);
            const class0Count = neighbors.filter(n => n.label === 0).length;
            const class1Count = neighbors.filter(n => n.label === 1).length;
            newPoint.label = class0Count > class1Count ? 0 : 1;

            datasets.push({
                label: 'New Point',
                data: [newPoint],
                backgroundColor: newPoint.label === 0 ? 'rgba(59, 130, 246, 1)' : 'rgba(239, 68, 68, 1)',
                borderColor: 'rgba(16, 185, 129, 1)',
                pointRadius: 8,
                borderWidth: 3,
            });
        }

        if (knnChart) {
            knnChart.data.datasets = datasets;
            knnChart.update();
        } else {
            knnChart = new Chart(knnCanvas, {
                type: 'scatter',
                data: { datasets },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: { min: 0, max: 10 },
                        y: { min: 0, max: 10 }
                    },
                    plugins: {
                        legend: { position: 'top' },
                        tooltip: { enabled: false }
                    }
                }
            });
        }
    }

    knnCanvas.onclick = (evt) => {
        const rect = knnCanvas.getBoundingClientRect();
        const x = (evt.clientX - rect.left) / rect.width * 10;
        const y = 10 - ((evt.clientY - rect.top) / rect.height * 10);
        newPoint = { x, y };
        drawKnnChart();
    };

    kSlider.oninput = drawKnnChart;
    resetKnnBtn.onclick = () => {
        generateKnnData();
        drawKnnChart();
    };

    // K-Means Chart
    let kmeansChart;
    const kmeansCanvas = document.getElementById('kmeansChart');
    const runKmeansBtn = document.getElementById('runKmeansBtn');
    let kmeansPoints = [];
    let centroids = [];
    const K = 3;

    function generateKmeansData() {
        kmeansPoints = [];
        for (let i = 0; i < 100; i++) {
            kmeansPoints.push({ x: Math.random() * 10, y: Math.random() * 10, cluster: -1 });
        }
        centroids = [];
        for (let i = 0; i < K; i++) {
            centroids.push({ x: Math.random() * 10, y: Math.random() * 10 });
        }
    }

    function drawKmeansChart(showCentroids = true) {
        const colors = ['rgba(59, 130, 246, 0.5)', 'rgba(239, 68, 68, 0.5)', 'rgba(16, 185, 129, 0.5)'];
        const borderColors = ['rgba(59, 130, 246, 1)', 'rgba(239, 68, 68, 1)', 'rgba(16, 185, 129, 1)'];
        
        const datasets = kmeansPoints.reduce((acc, point) => {
            const clusterIndex = point.cluster < 0 ? 0 : point.cluster;
            if (!acc[clusterIndex]) {
                 acc[clusterIndex] = {
                    label: `Cluster ${clusterIndex}`,
                    data: [],
                    backgroundColor: point.cluster < 0 ? 'rgba(107, 114, 128, 0.5)' : colors[clusterIndex],
                    pointRadius: 5
                };
            }
            acc[clusterIndex].data.push(point);
            return acc;
        }, []);

        if (showCentroids) {
            centroids.forEach((c, i) => {
                datasets.push({
                    label: `Centroid ${i}`,
                    data: [c],
                    backgroundColor: borderColors[i],
                    pointRadius: 8,
                    pointStyle: 'rectRot',
                });
            });
        }

        if (kmeansChart) {
            kmeansChart.data.datasets = datasets.filter(d => d);
            kmeansChart.update('none');
        } else {
            kmeansChart = new Chart(kmeansCanvas, {
                type: 'scatter',
                data: { datasets: datasets.filter(d => d) },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: { min: 0, max: 10 },
                        y: { min: 0, max: 10 }
                    },
                    plugins: { legend: { display: false } }
                }
            });
        }
    }

    async function runKmeans() {
        runKmeansBtn.disabled = true;
        generateKmeansData();
        drawKmeansChart();
        await new Promise(r => setTimeout(r, 500));

        for (let iter = 0; iter < 10; iter++) {
            // Assignment step
            kmeansPoints.forEach(p => {
                let minDist = Infinity;
                let bestCluster = -1;
                centroids.forEach((c, i) => {
                    const dist = (p.x - c.x) ** 2 + (p.y - c.y) ** 2;
                    if (dist < minDist) {
                        minDist = dist;
                        bestCluster = i;
                    }
                });
                p.cluster = bestCluster;
            });
            drawKmeansChart();
            await new Promise(r => setTimeout(r, 800));

            // Update step
            const newCentroids = Array(K).fill(0).map(() => ({ x: 0, y: 0, count: 0 }));
            kmeansPoints.forEach(p => {
                newCentroids[p.cluster].x += p.x;
                newCentroids[p.cluster].y += p.y;
                newCentroids[p.cluster].count++;
            });
            
            let moved = false;
            newCentroids.forEach((c, i) => {
                if (c.count > 0) {
                    const newX = c.x / c.count;
                    const newY = c.y / c.count;
                    if(Math.abs(newX - centroids[i].x) > 0.01 || Math.abs(newY - centroids[i].y) > 0.01) {
                        moved = true;
                    }
                    centroids[i] = { x: newX, y: newY };
                }
            });
            drawKmeansChart();
            await new Promise(r => setTimeout(r, 800));
            if (!moved) break;
        }
        runKmeansBtn.disabled = false;
    }
    runKmeansBtn.onclick = runKmeans;

    // Bias-Variance Chart
    const biasVarianceData = {
        labels: ['Low', '', '', '', 'Medium', '', '', '', 'High'],
        datasets: [
            {
                label: 'Bias',
                data: [9, 8, 7, 5, 3, 2, 1.5, 1, 0.8],
                borderColor: 'rgba(59, 130, 246, 1)',
                borderWidth: 3,
                tension: 0.4,
                fill: false,
            },
            {
                label: 'Variance',
                data: [0.5, 1, 1.5, 2, 3, 4.5, 6, 8, 9.5],
                borderColor: 'rgba(239, 68, 68, 1)',
                borderWidth: 3,
                tension: 0.4,
                fill: false,
            },
            {
                label: 'Total Error',
                data: [9.5, 9, 8.5, 7, 6, 6.5, 7.5, 9, 10.3],
                borderColor: 'rgba(16, 185, 129, 1)',
                borderWidth: 3,
                borderDash: [5, 5],
                tension: 0.4,
                fill: false,
            }
        ]
    };

    new Chart(document.getElementById('biasVarianceChart'), {
        type: 'line',
        data: biasVarianceData,
        options: {
            responsive: true,
            maintainAspectRatio: false,
            scales: {
                x: { title: { display: true, text: 'Model Complexity' } },
                y: { title: { display: true, text: 'Error' }, min: 0 }
            },
            plugins: { legend: { position: 'top' } }
        }
    });

    new Chart(document.getElementById('biasVarianceChartSmall'), {
        type: 'line',
        data: biasVarianceData,
        options: {
            responsive: true,
            maintainAspectRatio: false,
            scales: {
                x: { display: false },
                y: { display: false }
            },
            plugins: { legend: { display: false } }
        }
    });


    // Initial draws
    generateKnnData();
    drawKnnChart();
    generateKmeansData();
    drawKmeansChart(false);
});
</script>

</body>
</html>
