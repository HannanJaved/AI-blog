<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>ML Crash Course Blog</title>
  <style>
    :root {
      --bg: #0b0f14;
      --panel: #111725;
      --text: #e7ecf3;
      --muted: #aab3c2;
      --accent: #5cc8ff;
      --accent-2: #9b7bff;
      --ok: #34d399;
      --warn: #fbbf24;
      --err: #f87171;
    }
    * { box-sizing: border-box; }
    html, body { margin: 0; height: 100%; background: var(--bg); color: var(--text); font: 16px/1.55 system-ui, -apple-system, Segoe UI, Roboto, Ubuntu, "Helvetica Neue", Arial, "Noto Sans", "Apple Color Emoji", "Segoe UI Emoji"; }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }
    .container { max-width: 1100px; margin: 0 auto; padding: 24px; }
    header { position: sticky; top: 0; z-index: 50; backdrop-filter: blur(10px); background: linear-gradient(180deg, rgba(11,15,20,.9), rgba(11,15,20,.6)); border-bottom: 1px solid rgba(255,255,255,.06); }
    .brand { display: flex; align-items: center; gap: 12px; }
    .logo { width: 32px; height: 32px; border-radius: 10px; background: linear-gradient(135deg, var(--accent), var(--accent-2)); box-shadow: 0 0 0 2px rgba(255,255,255,.06) inset; }
    nav { display: flex; gap: 14px; flex-wrap: wrap; }
    .nav-link { color: var(--muted); padding: 8px 12px; border-radius: 10px; display: inline-block; }
    .nav-link:hover { background: rgba(255,255,255,.06); color: var(--text); }
    main { padding-top: 16px; }
    section { margin: 40px 0; padding: 24px; background: var(--panel); border: 1px solid rgba(255,255,255,.06); border-radius: 16px; box-shadow: 0 10px 30px rgba(0,0,0,.25); }
    h1, h2, h3 { line-height: 1.2; margin: 0 0 12px; }
    h1 { font-size: 34px; }
    h2 { font-size: 26px; border-left: 4px solid var(--accent); padding-left: 10px; }
    h3 { font-size: 20px; color: var(--accent); margin-top: 18px; }
    p { margin: 10px 0; color: var(--text); }
    .muted { color: var(--muted); }
    .pill { display: inline-block; padding: 4px 10px; border-radius: 999px; background: rgba(255,255,255,.06); color: var(--muted); font-size: 12px; margin-right: 6px; }
    .two-col { display: grid; grid-template-columns: repeat(auto-fit, minmax(260px, 1fr)); gap: 16px; }
    .card { background: #0e1422; border: 1px solid rgba(255,255,255,.06); padding: 14px; border-radius: 12px; }
    .code { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "DejaVu Sans Mono", monospace; background: #0a0f1a; border: 1px solid rgba(255,255,255,.06); padding: 12px; border-radius: 10px; overflow-x: auto; }
    .note { border-left: 4px solid var(--accent-2); background: rgba(155,123,255,.08); padding: 10px 12px; border-radius: 8px; }
    .ok { color: var(--ok); }
    .warn { color: var(--warn); }
    .err { color: var(--err); }
    footer { color: var(--muted); text-align: center; padding: 30px 0 60px; }
    .toc { display:flex; gap:10px; flex-wrap:wrap; }
    .toc a { background:#0e1422; border:1px solid rgba(255,255,255,.06); padding:6px 10px; border-radius:999px; }
  </style>
</head>
<body>
  <header>
    <div class="container" style="display:flex; align-items:center; gap:20px; justify-content:space-between;">
      <div class="brand"><div class="logo" aria-hidden="true"></div><div>
        <div style="font-weight:700;">ML Crash Course Blog</div>
        <div class="muted" style="font-size:12px;">A short crash-course on the fundamentals of Machine Learning</div>
      </div></div>
      <nav>
        <a class="nav-link" href="#module1">Intro</a>
        <a class="nav-link" href="#module2">Regression</a>
        <a class="nav-link" href="#module3">Classification</a>
        <a class="nav-link" href="#module4">Unsupervised</a>
        <a class="nav-link" href="#module5">Training & Eval</a>
        <a class="nav-link" href="#module6">Advanced & DL</a>
      </nav>
    </div>
  </header>
  <main class="container">
    <section id="module1">
      <h1>Introduction to Machine Learning</h1>
      <p class="muted">High-level overview: generalization, learning paradigms (supervised, unsupervised, self-supervised, reinforcement), and the ML workflow.</p>
      <div class="toc">
        <a href="#module2">Regression</a>
        <a href="#module3">Classification</a>
        <a href="#module4">Unsupervised</a>
        <a href="#module5">Training & Eval</a>
        <a href="#module6">Advanced & DL</a>
      </div>
    </section>
    <section id="module2">
      <h2>Supervised Learning – Regression</h2>
      <p>Regression models predict continuous values. Examples include linear regression (line of best fit, minimize MSE), polynomial regression for non-linear curves.</p>
      <div class="card">
        <h3>Linear Regression</h3>
        <p>Equation: ŷ = β₀ + β₁x₁ + … + βₚxₚ. Loss function: Mean Squared Error (MSE). Parameters learned by normal equation or gradient descent.</p>
      </div>
      <div class="card">
        <h3>Polynomial Regression</h3>
        <p>Adds polynomial terms (x², x³…) to capture non-linear relationships.</p>
      </div>
    </section>
    <section id="module3">
      <h2>Supervised Learning – Classification</h2>
      <div class="card">
        <h3>Logistic Regression</h3>
        <p>Predicts probability via sigmoid. Loss: Binary cross-entropy. Equation: p̂ = 1 / (1+e^{-(β₀+β₁x₁+…)})</p>
      </div>
      <div class="card">
        <h3>k-Nearest Neighbors</h3>
        <p>Classifies by majority vote of k closest neighbors. “Lazy learner” – no training, just distance computations at prediction.</p>
      </div>
      <div class="card">
        <h3>Support Vector Machines</h3>
        <p>Finds maximum-margin hyperplane separating classes. Support vectors define the margin.</p>
      </div>
      <div class="card">
        <h3>Decision Trees & Random Forests</h3>
        <p>Trees split recursively based on impurity measures. Random Forest = ensemble of trees with bagging and feature subsampling, reducing overfitting.</p>
      </div>
    </section>
    <section id="module4">
      <h2>Unsupervised Learning</h2>
      <div class="card">
        <h3>K-Means Clustering</h3>
        <p>Iteratively assigns points to k centroids and updates means. Strengths: simple, fast. Weaknesses: must choose k, sensitive to initialization.</p>
      </div>
      <div class="card">
        <h3>Principal Component Analysis (PCA)</h3>
        <p>Dimensionality reduction: computes covariance matrix, finds eigenvectors/eigenvalues, projects data onto top components.</p>
      </div>
    </section>
    <section id="module5">
      <h2>Model Training & Evaluation</h2>
      <div class="card">
        <h3>Evaluation Metrics</h3>
        <ul>
          <li>Classification: Precision, Recall, F1, ROC/AUC</li>
          <li>Regression: MAE, MSE</li>
        </ul>
      </div>
      <div class="card">
        <h3>Cross-Validation</h3>
        <p>k-fold CV provides robust performance estimates and avoids lucky/unlucky splits.</p>
      </div>
      <div class="card">
        <h3>Bias–Variance Tradeoff</h3>
        <p>Simple models underfit (high bias), complex models overfit (high variance). Goal: minimize total error.</p>
      </div>
      <div class="card">
        <h3>Regularization</h3>
        <p>L1 (Lasso) drives coefficients to zero (feature selection). L2 (Ridge) shrinks coefficients but rarely zero.</p>
      </div>
      <div class="card">
        <h3>Hyperparameter Tuning</h3>
        <p>Grid search (expensive), Random search (faster), Bayesian optimization (efficient for costly models).</p>
      </div>
    </section>
    <section id="module6">
      <h2>Advanced Topics & Deep Learning</h2>
      <div class="card">
        <h3>Feed-Forward Neural Network</h3>
        <p>Layers of neurons (input → hidden → output) with non-linear activations allow learning of complex functions.</p>
      </div>
      <div class="card">
        <h3>How Neural Networks Learn</h3>
        <p>Forward pass, compute loss, backpropagate gradients, update weights with gradient descent.</p>
      </div>
      <div class="card">
        <h3>Bayes’ Theorem</h3>
        <p>P(A|B) = (P(B|A)P(A)) / P(B). Updates prior beliefs with new evidence to get posterior probabilities.</p>
      </div>
      <div class="card">
        <h3>Generative vs. Discriminative</h3>
        <p>Discriminative models: learn P(Y|X). Generative models: learn joint P(X,Y) and can generate new samples.</p>
      </div>
    </section>
    <footer>
      <p>Built for quick study and onboarding. Keep iterating.</p>
    </footer>
  </main>
</body>
</html>
