<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The ML Crash Course Blog</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"
      async
    ></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f5f5f4; /* stone-100 */
            color: #292524; /* stone-800 */
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 350px;
            max-height: 400px;
        }
        .static-chart-container {
             position: relative;
            width: 100%;
            max-width: 500px;
            margin-left: auto;
            margin-right: auto;
            height: 300px;
            max-height: 350px;
        }
        .formula {
            font-family: 'Times New Roman', serif;
            background-color: #e7e5e4; /* stone-200 */
            padding: 1rem;
            border-radius: 0.5rem;
            text-align: center;
            overflow-x: auto;
            font-size: 1.2rem;
            margin: 1rem 0;
            color: #44403c; /* stone-700 */
        }
    </style>
</head>
<body class="antialiased">

    <header class="bg-white shadow-md">
        <div class="max-w-7xl mx-auto py-6 px-4 sm:px-6 lg:px-8">
            <h1 class="text-3xl font-bold text-violet-700">The ML Crash Course Blog</h1>
            <p class="mt-1 text-stone-600">A detailed and interactive guide to the fundamentals of Machine Learning.</p>
        </div>
    </header>

    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8">
        <div class="lg:flex lg:space-x-8">
            <main class="flex-1">
                <div class="space-y-16">
                    <article id="module1" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-violet-700 border-b-2 border-violet-200 pb-2 mb-6">Module 1: Introduction to Machine Learning</h2>
                        <div class="space-y-8 text-lg text-stone-700">
                            <p>This module provides a high-level overview of machine learning, establishing the core goal of generalization and introducing the main paradigms of learning. It sets the stage for understanding not just what ML is, but why it's structured the way it is.</p>
                            
                            <div>
                                <h3 class="text-2xl font-semibold mt-4 text-stone-800">1.1. What is Machine Learning?</h3>
                                <p class="mt-2">At its core, Machine Learning is the science of getting computers to act without being explicitly programmed. Instead of writing a fixed set of rules, we provide algorithms with large amounts of data and let them learn the rules and patterns for themselves. The formal definition by Tom Mitchell provides a more structured view:</p>
                                <blockquote class="mt-4 p-4 bg-stone-100 border-l-4 border-violet-500">
                                    "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."
                                </blockquote>
                                <p class="mt-4">Let's break this down with a real-world example: a spam filter.</p>
                                <ul class="list-disc list-inside mt-2 space-y-1 text-base">
                                    <li><strong>Task (T):</strong> To classify emails as either "spam" or "not spam".</li>
                                    <li><strong>Experience (E):</strong> A massive dataset of emails, where each one has already been labeled by humans as spam or not spam.</li>
                                    <li><strong>Performance (P):</strong> The percentage of emails the program correctly classifies.</li>
                                </ul>
                                <p class="mt-2">The ultimate goal is **Generalization**: the ability of the model to perform accurately on new, unseen emails it has never encountered before, not just the ones it was trained on.</p>
                            </div>

                            <div>
                                <h3 class="text-2xl font-semibold mt-4 text-stone-800">1.2. The Pillars of ML</h3>
                                <div class="space-y-6 mt-4">
                                    <div>
                                        <h4 class="font-semibold text-violet-600">Supervised Learning</h4>
                                        <p class="text-base text-stone-600 mb-2">Think of this as learning with a teacher or an answer key. The model is given a dataset where each data point is tagged with a correct label or output. The goal is to learn a mapping function, $f$, such that $y \approx f(x)$.</p>
                                    </div>
                                    <div>
                                        <h4 class="font-semibold text-violet-600">Unsupervised Learning</h4>
                                        <p class="text-base text-stone-600 mb-2">This is learning without an answer key. The model is given data without explicit labels and must find structure or patterns on its own, such as grouping data into clusters.</p>
                                    </div>
                                     <div>
                                        <h4 class="font-semibold text-violet-600">Self-Supervised Learning</h4>
                                        <p class="text-base text-stone-600 mb-2">This is a clever type of supervised learning where the labels are generated from the input data itself. For example, you could take a sentence, remove a word, and then train a model to predict that missing word. The "label" is the word you removed, but you didn't need a human to create it. This allows models to learn from vast amounts of unlabeled text or image data.</p>
                                    </div>
                                    <div>
                                        <h4 class="font-semibold text-violet-600">Reinforcement Learning</h4>
                                        <p class="text-base text-stone-600 mb-2">This is learning through trial and error. An "agent" learns to make decisions by performing actions in an "environment" to maximize a cumulative reward. The objective is to learn a policy, $\pi$, that maximizes the expected discounted return: $R = \sum_{t=0}^{T} \gamma^t r_t$, where $\gamma$ is the discount factor and $r_t$ is the reward at time $t$.</p>
                                    </div>
                                </div>
                            </div>
                            
                            <div>
                                <h3 class="text-2xl font-semibold mt-8 text-stone-800">1.3. The ML Workflow</h3>
                                <p class="mt-2">Nearly every machine learning project follows a standard, iterative lifecycle. Understanding these steps is crucial for building effective models.</p>
                                <ol class="list-decimal list-inside mt-4 space-y-4 text-base">
                                    <li><strong>Problem Framing:</strong> Define the objective. What question are you trying to answer? Is it a regression problem (predicting a number) or a classification problem (predicting a category)? What metric will define success?</li>
                                    <li><strong>Data Collection & Preprocessing:</strong> Gather the necessary data. This is often the most time-consuming step, involving cleaning the data (handling missing values, outliers), feature engineering (creating new input signals), and scaling features to a common range.</li>
                                    <li><strong>Model Selection & Training:</strong> Choose a suitable algorithm and feed it the prepared training data. The model learns the underlying patterns during this "training" phase.</li>
                                    <li><strong>Evaluation:</strong> Assess the model's performance on a held-out test setâ€”data it has never seen before. This gives an unbiased estimate of how it will perform in the real world.</li>
                                    <li><strong>Hyperparameter Tuning:</strong> Adjust the model's settings (hyperparameters, like the 'k' in k-NN) to improve performance, often using a validation dataset to prevent overfitting to the test set.</li>
                                    <li><strong>Deployment:</strong> Integrate the trained model into a production environment where it can make predictions on new, live data.</li>
                                </ol>
                            </div>
                        </div>
                    </article>

                    <article id="module2" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-violet-700 border-b-2 border-violet-200 pb-2 mb-6">Module 2: Supervised Learning - Regression</h2>
                         <p class="text-lg text-stone-700 mb-6">Regression models are used when the goal is to predict a continuous numerical value. This module focuses on the foundational algorithms for this task.</p>
                        <div class="space-y-8">
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">2.1. Linear Regression</h3>
                                <p>Fits a "line of best fit" to the data. It models a linear relationship between input features (X) and a continuous target variable (y).</p>
                                <div class="formula">$\hat{y} = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p$</div>
                                 <h4 class="font-semibold mt-4 text-stone-700">The Loss Function:</h4>
                                <p>The model learns by minimizing the **Mean Squared Error (MSE)**:</p>
                                <div class="formula">$L(\beta) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$</div>
                                <h4 class="font-semibold mt-4 text-stone-700">How it Learns: Finding the Parameters</h4>
                                <p>There are two primary methods to find the optimal parameters ($\beta$) that minimize the MSE loss:</p>
                                <ol class="list-decimal list-inside mt-2 space-y-2 text-base">
                                    <li><strong>Closed-Form Solution (Normal Equation):</strong> This is a direct mathematical formula that calculates the optimal parameters in one step. It's efficient for smaller datasets but computationally expensive for datasets with many features.
                                    <div class="formula">$\beta = (X^T X)^{-1} X^T y$</div>
                                    </li>
                                    <li><strong>Gradient Descent:</strong> An iterative optimization algorithm. It starts with random parameters and repeatedly adjusts them in the direction that most reduces the loss. The update rule for a single parameter $\beta_j$ is:
                                    <div class="formula">$\beta_{j} := \beta_{j} - \alpha \frac{\partial}{\partial \beta_{j}} L(\beta)$</div>
                                    For Linear Regression, the MSE loss function is **convex**, meaning it has only one global minimum. Therefore, gradient descent will **always converge to the same optimal solution**.</li>
                                </ol>
                                <div class="static-chart-container mt-4">
                                    <canvas id="linearRegressionChart"></canvas>
                                </div>
                            </div>
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">2.2. Polynomial Regression</h3>
                                <p>An extension of linear regression that can model non-linear relationships by adding polynomial terms of the features to the model.</p>
                                <div class="formula">$\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \dots$</div>
                                 <h4 class="font-semibold mt-4 text-stone-700">Why it's useful:</h4>
                                <p>Many real-world relationships are not straight lines. For example, the relationship between a car's speed and its fuel efficiency is curved. Polynomial regression allows us to fit a more flexible, curved line to such data, leading to a more accurate model.</p>
                                <div class="static-chart-container mt-4">
                                    <canvas id="polynomialRegressionChart"></canvas>
                                </div>
                            </div>
                        </div>
                    </article>

                    <article id="module3" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-violet-700 border-b-2 border-violet-200 pb-2 mb-6">Module 3: Supervised Learning - Classification</h2>
                         <p class="text-lg text-stone-700 mb-6">Classification models are used when the goal is to predict a discrete category or class label. This module covers the foundational algorithms for this task.</p>
                        
                        <div class="space-y-8">
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">3.1. Logistic Regression</h3>
                                <p>A baseline for classification that predicts the probability an input belongs to a class. It passes a linear equation through a Sigmoid function, which squashes the output to a range between 0 and 1.</p>
                                <div class="formula">$z = \beta_0 + \beta_1 x_1 + \dots; \quad \hat{p} = \sigma(z) = \frac{1}{1 + e^{-z}}$</div>
                                 <h4 class="font-semibold mt-4 text-stone-700">How it Learns:</h4>
                                <p>It minimizes a loss function called **Log Loss (or Binary Cross-Entropy)**, which penalizes confident but incorrect predictions heavily.</p>
                                 <div class="formula">$L(\beta) = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i)]$</div>
                                 <div class="static-chart-container mt-4">
                                    <canvas id="logisticRegressionChart"></canvas>
                                 </div>
                            </div>
                            
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">3.2. k-Nearest Neighbors (k-NN) Interactive Demo</h3>
                                <p class="mb-4 text-stone-700">A "lazy" algorithm that classifies a new point based on the majority vote of its 'k' closest neighbors. It uses a distance metric, like Euclidean distance, to find the neighbors.</p>
                                <div class="formula">$d(p, q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}$</div>
                                <div class="chart-container">
                                    <canvas id="knnChart"></canvas>
                                </div>
                                <div class="mt-4 flex items-center justify-center space-x-4">
                                    <label for="kSlider" class="font-medium text-stone-700">k = <span id="kValue">3</span></label>
                                    <input type="range" id="kSlider" min="1" max="15" value="3" step="2" class="w-64">
                                     <button id="resetKnnBtn" class="px-4 py-2 bg-violet-600 text-white rounded-lg hover:bg-violet-700 transition">Reset</button>
                                </div>
                            </div>

                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">3.3. Support Vector Machines (SVMs)</h3>
                                <p>A "maximum margin" classifier that finds the hyperplane that best separates the classes by maximizing the distance to the nearest points of any class (the support vectors).</p>
                                 <h4 class="font-semibold mt-4 text-stone-700">How it Learns:</h4>
                                <p>SVMs solve a constrained optimization problem to find the parameters ($w, b$) of the hyperplane. The objective is to minimize the magnitude of the margin while ensuring all points are correctly classified.</p>
                                <div class="formula">$\min_{w,b} \frac{1}{2} ||w||^2 \quad \text{subject to} \quad y_i(w \cdot x_i - b) \geq 1$</div>
                                <div class="static-chart-container mt-4">
                                    <canvas id="svmChart"></canvas>
                                </div>
                            </div>

                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">3.4. Decision Trees & Random Forests</h3>
                                <p>A flowchart of "if-then-else" questions that splits the data into purer subgroups.</p>
                                 <h4 class="font-semibold mt-4 text-stone-700">How it Learns:</h4>
                                <p>The tree is built via **recursive partitioning**, seeking splits that maximize **Information Gain** or minimize impurity. A common impurity measure is **Gini Impurity**, which measures the likelihood of misclassifying a randomly chosen element.</p>
                                <div class="formula">$G_i = 1 - \sum_{k=1}^{c} p_{i,k}^2$</div>
                                <div class="my-6">
                                    <img src="https://i.imgur.com/uRKYs3W.png" alt="Decision tree example for predicting whether to play outside based on weather" class="mx-auto rounded-lg shadow-md">
                                    <p class="text-center text-sm text-stone-500 mt-2">Image Source: databasecamp.de</p>
                                </div>
                                <h4 class="font-semibold mt-6 text-stone-700">How Random Forests Improve on Decision Trees:</h4>
                                <p>A single decision tree can easily overfit. **Random Forests** build an ensemble of trees on random subsets of data and features. The final prediction is a majority vote, which reduces variance and improves generalization.</p>
                            </div>
                        </div>
                    </article>

                    <article id="module4" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-violet-700 border-b-2 border-violet-200 pb-2 mb-6">Module 4: Unsupervised Learning</h2>
                        <p class="text-lg text-stone-700 mb-6">Unsupervised learning deals with unlabeled data, seeking to find hidden patterns or intrinsic structures.</p>
                        
                         <div class="space-y-8">
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">4.1. K-Means Clustering Interactive Demo</h3>
                                <p>An algorithm that partitions data into 'k' clusters by minimizing the within-cluster sum of squares (WCSS).</p>
                                <div class="formula">$\text{WCSS} = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$</div>
                                <p>It uses an iterative **Expectation-Maximization (E-M)** algorithm: 1) **Assignment Step:** Assign each point to the nearest centroid ($\mu_i$). 2) **Update Step:** Recalculate centroids as the mean of assigned points.</p>
                                <div class="chart-container">
                                    <canvas id="kmeansChart"></canvas>
                                </div>
                                <div class="mt-4 flex items-center justify-center space-x-4">
                                    <label for="kMeansSlider" class="font-medium text-stone-700">k = <span id="kMeansValue">3</span></label>
                                    <input type="range" id="kMeansSlider" min="2" max="8" value="3" step="1" class="w-64">
                                    <button id="runKmeansBtn" class="px-4 py-2 bg-violet-600 text-white rounded-lg hover:bg-violet-700 transition">Run K-Means</button>
                                </div>
                            </div>

                             <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">4.2. Principal Component Analysis (PCA)</h3>
                                <p>A dimensionality reduction technique that transforms data into a new coordinate system of orthogonal axes (principal components) that capture the maximum variance.</p>
                                <h4 class="font-semibold mt-4 text-stone-700">How it Works:</h4>
                                <p>PCA computes the eigenvectors and eigenvalues of the data's covariance matrix ($\Sigma$). An eigenvector $v$ and its eigenvalue $\lambda$ satisfy:</p>
                                <div class="formula">$\Sigma v = \lambda v$</div>
                                <p>The eigenvectors (principal components) point in the directions of maximum variance, and the corresponding eigenvalues indicate the amount of variance captured.</p>
                            </div>
                        </div>
                    </article>

                    <article id="module5" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-violet-700 border-b-2 border-violet-200 pb-2 mb-6">Module 5: Model Training & Evaluation</h2>
                        <p class="text-lg text-stone-700 mb-6">Building a model is only half the battle; we must also rigorously evaluate its performance and understand its limitations.</p>

                        <div class="space-y-8">
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">5.1. Key Metrics for Classification</h3>
                                <p>We use a **Confusion Matrix** to derive several key metrics:</p>
                                <ul class="list-disc list-inside mt-2 space-y-2">
                                    <li><strong>Precision:</strong> "Of all positive predictions, how many were right?" High precision is vital when false positives are costly.
                                    <div class="formula text-base">$\text{Precision} = \frac{TP}{TP + FP}$</div>
                                    </li>
                                    <li><strong>Recall:</strong> "Of all actual positives, how many did we find?" High recall is vital when false negatives are costly.
                                    <div class="formula text-base">$\text{Recall} = \frac{TP}{TP + FN}$</div>
                                    </li>
                                    <li><strong>F1-Score:</strong> The harmonic mean of Precision and Recall, balancing both.
                                    <div class="formula text-base">$F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$</div>
                                    </li>
                                </ul>
                            </div>
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">5.2. Cross-Validation</h3>
                                <p>**k-fold cross-validation** provides a robust estimate of model performance by splitting data into 'k' folds, training on k-1, and testing on the remaining one, repeating this 'k' times and averaging the results. This ensures the performance metric isn't a fluke from a single lucky train-test split.</p>
                            </div>
                             <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">5.3. The Bias-Variance Tradeoff</h3>
                                <p class="mb-4 text-stone-700">This is the central challenge in supervised learning. The total error of a model is a sum of its bias, variance, and irreducible error.</p>
                                <div class="formula">$E[(y - \hat{f}(x))^2] = (\text{Bias}[\hat{f}(x)])^2 + \text{Var}[\hat{f}(x)] + \sigma^2$</div>
                                <ul class="list-disc list-inside text-base space-y-1">
                                    <li><b>High Bias (Underfitting):</b> The model is too simple and makes systematic errors.</li>
                                    <li><b>High Variance (Overfitting):</b> The model is too complex and captures noise in the training data.</li>
                                </ul>
                                <div class="static-chart-container">
                                    <canvas id="biasVarianceChart"></canvas>
                                </div>
                            </div>
                            
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">5.4. Regularization</h3>
                                <p>Regularization prevents overfitting by adding a penalty term to the loss function, discouraging large parameter values.</p>
                                <div class="grid md:grid-cols-2 gap-8 mt-4 items-center">
                                    <div>
                                        <h4 class="font-semibold text-violet-600">L1 (Lasso) Regularization</h4>
                                        <p class="text-sm text-stone-600 mb-2">Adds a penalty proportional to the absolute value of the coefficients. This can shrink some coefficients to exactly zero, performing feature selection.</p>
                                        <div class="formula text-base">$L_{Lasso} = \text{MSE} + \lambda \sum_{j=1}^{p} |\beta_j|$</div>
                                        
                                        <h4 class="font-semibold text-violet-600 mt-4">L2 (Ridge) Regularization</h4>
                                        <p class="text-sm text-stone-600 mb-2">Adds a penalty proportional to the square of the coefficients. This shrinks coefficients but rarely to zero.</p>
                                        <div class="formula text-base">$L_{Ridge} = \text{MSE} + \lambda \sum_{j=1}^{p} \beta_j^2$</div>
                                    </div>
                                    <div>
                                        <img src="https://i.imgur.com/Bf11nAn.png" alt="Geometric interpretation of L1 and L2 regularization" class="mx-auto rounded-lg">
                                        <p class="text-center text-sm text-stone-500 mt-2">Image Source: Wikimedia Commons</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </article>
                    
                    <article id="module6" class="bg-white p-6 sm:p-8 rounded-lg shadow-sm scroll-mt-20">
                        <h2 class="text-3xl font-bold text-violet-700 border-b-2 border-violet-200 pb-2 mb-6">Module 6: Advanced Topics & Deep Learning</h2>
                        <div class="space-y-8">
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">6.1. Feed-Forward Neural Network</h3>
                                <p>A model inspired by the brain, composed of interconnected "neurons" in layers. A neuron's output is an activation function, $g$, applied to the weighted sum of its inputs.</p>
                                <div class="formula">$a = g(w \cdot x + b) = g\left(\sum_i w_i x_i + b\right)$</div>
                                <div class="flex justify-center items-center space-x-8 p-4 mt-4 border-t border-stone-200">
                                    <div class="flex flex-col items-center space-y-4"><p class="font-semibold">Input</p><div class="w-8 h-8 bg-blue-300 rounded-full"></div><div class="w-8 h-8 bg-blue-300 rounded-full"></div></div>
                                    <p class="text-4xl font-thin">â†’</p>
                                    <div class="flex flex-col items-center space-y-4"><p class="font-semibold">Hidden Layer</p><div class="w-8 h-8 bg-violet-300 rounded-full"></div><div class="w-8 h-8 bg-violet-300 rounded-full"></div><div class="w-8 h-8 bg-violet-300 rounded-full"></div></div>
                                    <p class="text-4xl font-thin">â†’</p>
                                    <div class="flex flex-col items-center space-y-4"><p class="font-semibold">Output</p><div class="w-8 h-8 bg-red-300 rounded-full"></div></div>
                                </div>
                            </div>
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">6.2. How Neural Networks Learn</h3>
                                <p>Networks learn by minimizing a loss function using **Gradient Descent** and **Backpropagation**. Backpropagation efficiently computes the gradient of the loss with respect to each weight ($w$) and bias ($b$), and Gradient Descent updates the parameters to minimize the loss.</p>
                                <div class="formula">$w_{ij} := w_{ij} - \alpha \frac{\partial L}{\partial w_{ij}}$</div>
                            </div>
                            <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">6.3. Foundational Concepts - Bayes' Theorem</h3>
                                <p class="mb-4">Bayes' Theorem is a mathematical way to update beliefs based on new evidence. It relates the posterior probability of a hypothesis to its prior probability and the likelihood of the evidence.</p>
                                <div class="formula">$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$</div>
                            </div>
                             <div>
                                <h3 class="text-2xl font-semibold mb-2 text-stone-800">6.4. Generative vs. Discriminative Models</h3>
                                <div class="grid md:grid-cols-2 gap-6">
                                    <div>
                                        <h4 class="font-semibold text-violet-600">Discriminative Models</h4>
                                        <p>Learn the decision boundary between classes by modeling the conditional probability $P(y|x)$ directly.</p>
                                        <p class="text-sm text-stone-500">Examples: Logistic Regression, SVMs, standard Neural Networks.</p>
                                    </div>
                                    <div>
                                        <h4 class="font-semibold text-violet-600">Generative Models</h4>
                                        <p>Learn the underlying distribution of the data for each class by modeling the joint probability $P(x,y)$, often by learning $P(x|y)$ and $P(y)$.</p>
                                        <p class="text-sm text-stone-500">Examples: Naive Bayes, GANs.</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </article>
                </div>
            </main>

            <aside class="w-80 hidden lg:block">
                <div class="sticky top-8 space-y-8">
                    <div class="bg-white p-6 rounded-lg shadow-sm">
                        <h3 class="text-xl font-semibold text-stone-800 mb-4">Index</h3>
                        <nav id="sidebar-nav" class="space-y-3">
                            <a href="#module1" class="block font-medium text-stone-700 hover:text-violet-600">1. Introduction to ML</a>
                            <a href="#module2" class="block font-medium text-stone-700 hover:text-violet-600">2. Supervised: Regression</a>
                            <a href="#module3" class="block font-medium text-stone-700 hover:text-violet-600">3. Supervised: Classification</a>
                            <a href="#module4" class="block font-medium text-stone-700 hover:text-violet-600">4. Unsupervised Learning</a>
                            <a href="#module5" class="block font-medium text-stone-700 hover:text-violet-600">5. Training & Evaluation</a>
                            <a href="#module6" class="block font-medium text-stone-700 hover:text-violet-600">6. Advanced Topics & DL</a>
                        </nav>
                    </div>
                </div>
            </aside>
        </div>
    </div>

<script>
document.addEventListener('DOMContentLoaded', () => {

    // Navigation scroll highlighting
    const sections = document.querySelectorAll('article');
    const navLinks = document.querySelectorAll('#desktop-nav a, #sidebar-nav a');

    const observer = new IntersectionObserver((entries) => {
        entries.forEach(entry => {
            if (entry.isIntersecting) {
                const id = entry.target.id;
                navLinks.forEach(link => {
                    link.classList.toggle('text-violet-600', link.getAttribute('href') === `#${id}`);
                });
            }
        });
    }, { rootMargin: '-40% 0px -60% 0px', threshold: 0 });

    sections.forEach(section => observer.observe(section));

    // Static Linear Regression Chart
    new Chart(document.getElementById('linearRegressionChart'), {
        type: 'scatter',
        data: {
            datasets: [{
                label: 'Data Points',
                data: [{x:1, y:2}, {x:2, y:2.5}, {x:3, y:4}, {x:4, y:4.5}, {x:5, y:6}, {x:6, y:5.5}],
                backgroundColor: 'rgba(59, 130, 246, 0.5)',
            }, {
                label: 'Line of Best Fit',
                data: [{x:0, y:1}, {x:7, y:7}],
                type: 'line',
                borderColor: 'rgba(239, 68, 68, 1)',
                borderWidth: 2,
                fill: false,
                pointRadius: 0,
            }]
        },
        options: { responsive: true, maintainAspectRatio: false, scales: { x: { min: 0, max: 7 }, y: { min: 0, max: 8 } }, plugins: { legend: { display: false } } }
    });

    // Static Polynomial Regression Chart
    new Chart(document.getElementById('polynomialRegressionChart'), {
        type: 'scatter',
        data: {
            datasets: [{
                label: 'Data Points',
                data: [{x:0, y:1}, {x:1, y:2.5}, {x:2, y:3}, {x:3, y:2.5}, {x:4, y:3.5}, {x:5, y:6}, {x:6, y:8}],
                backgroundColor: 'rgba(59, 130, 246, 0.5)',
            }, {
                label: 'Polynomial Fit',
                data: Array.from({length: 61}, (_, i) => {
                    const x = i * 0.1;
                    const y = 0.1 * (x**2) + 0.5 * x + 1.5;
                    return {x, y};
                }),
                type: 'line',
                borderColor: 'rgba(16, 185, 129, 1)',
                borderWidth: 2,
                fill: false,
                pointRadius: 0,
                tension: 0.1
            }]
        },
        options: { responsive: true, maintainAspectRatio: false, scales: { x: { min: 0, max: 7 }, y: { min: 0, max: 9 } }, plugins: { legend: { display: false } } }
    });

    // Static Logistic Regression Chart
    new Chart(document.getElementById('logisticRegressionChart'), {
        type: 'scatter',
        data: {
            datasets: [{
                label: 'Class 0',
                data: [{x:1, y:0}, {x:2, y:0}, {x:3, y:0}, {x:4, y:0}],
                backgroundColor: 'rgba(59, 130, 246, 0.5)',
                pointRadius: 6
            }, {
                label: 'Class 1',
                data: [{x:6, y:1}, {x:7, y:1}, {x:8, y:1}, {x:9, y:1}],
                backgroundColor: 'rgba(239, 68, 68, 0.5)',
                pointRadius: 6
            }, {
                label: 'Sigmoid Function',
                data: Array.from({length: 100}, (_, i) => {
                    const x = i * 0.1;
                    const y = 1 / (1 + Math.exp(-(x - 5)));
                    return {x, y};
                }),
                type: 'line',
                borderColor: 'rgba(16, 185, 129, 1)',
                borderWidth: 2,
                fill: false,
                pointRadius: 0,
                tension: 0.1
            }]
        },
        options: { responsive: true, maintainAspectRatio: false, scales: { x: { min: 0, max: 10 }, y: { min: -0.1, max: 1.1 } }, plugins: { legend: { display: false } } }
    });


    // k-NN Chart
    let knnChart;
    const knnCanvas = document.getElementById('knnChart');
    const kSlider = document.getElementById('kSlider');
    const kValueSpan = document.getElementById('kValue');
    const resetKnnBtn = document.getElementById('resetKnnBtn');
    let knnPoints = [];
    let newPoint = null;

    function generateKnnData() {
        knnPoints = [];
        for (let i = 0; i < 20; i++) {
            knnPoints.push({ x: Math.random() * 5, y: Math.random() * 5, label: 0 });
            knnPoints.push({ x: Math.random() * 5 + 5, y: Math.random() * 5 + 5, label: 1 });
        }
        newPoint = null;
    }

    function drawKnnChart() {
        const k = parseInt(kSlider.value);
        kValueSpan.textContent = k;

        const datasets = [{
            label: 'Class 0',
            data: knnPoints.filter(p => p.label === 0),
            backgroundColor: 'rgba(59, 130, 246, 0.5)',
            borderColor: 'rgba(59, 130, 246, 1)',
            pointRadius: 6,
        }, {
            label: 'Class 1',
            data: knnPoints.filter(p => p.label === 1),
            backgroundColor: 'rgba(239, 68, 68, 0.5)',
            borderColor: 'rgba(239, 68, 68, 1)',
            pointRadius: 6,
        }];

        if (newPoint) {
            const distances = knnPoints.map(p => ({
                dist: Math.sqrt((p.x - newPoint.x) ** 2 + (p.y - newPoint.y) ** 2),
                label: p.label
            }));
            distances.sort((a, b) => a.dist - b.dist);
            const neighbors = distances.slice(0, k);
            const class0Count = neighbors.filter(n => n.label === 0).length;
            const class1Count = neighbors.filter(n => n.label === 1).length;
            newPoint.label = class0Count > class1Count ? 0 : 1;

            datasets.push({
                label: 'New Point',
                data: [newPoint],
                backgroundColor: newPoint.label === 0 ? 'rgba(59, 130, 246, 1)' : 'rgba(239, 68, 68, 1)',
                borderColor: 'rgba(16, 185, 129, 1)',
                pointRadius: 8,
                borderWidth: 3,
            });
        }

        if (knnChart) {
            knnChart.data.datasets = datasets;
            knnChart.update();
        } else {
            knnChart = new Chart(knnCanvas, {
                type: 'scatter',
                data: { datasets },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: { min: 0, max: 10 },
                        y: { min: 0, max: 10 }
                    },
                    plugins: {
                        legend: { position: 'top' },
                        tooltip: { enabled: false }
                    }
                }
            });
        }
    }

    knnCanvas.onclick = (evt) => {
        const rect = knnCanvas.getBoundingClientRect();
        const x = (evt.clientX - rect.left) / rect.width * 10;
        const y = 10 - ((evt.clientY - rect.top) / rect.height * 10);
        newPoint = { x, y };
        drawKnnChart();
    };

    kSlider.oninput = drawKnnChart;
    resetKnnBtn.onclick = () => {
        generateKnnData();
        drawKnnChart();
    };

    // K-Means Chart
    let kmeansChart;
    const kmeansCanvas = document.getElementById('kmeansChart');
    const runKmeansBtn = document.getElementById('runKmeansBtn');
    const kMeansSlider = document.getElementById('kMeansSlider');
    const kMeansValueSpan = document.getElementById('kMeansValue');
    let kmeansPoints = [];
    let centroids = [];

    kMeansSlider.oninput = () => {
        kMeansValueSpan.textContent = kMeansSlider.value;
    };

    function generateKmeansData(k) {
        kmeansPoints = [];
        for (let i = 0; i < 100; i++) {
            kmeansPoints.push({ x: Math.random() * 10, y: Math.random() * 10, cluster: -1 });
        }
        centroids = [];
        for (let i = 0; i < k; i++) {
            centroids.push({ x: Math.random() * 10, y: Math.random() * 10 });
        }
    }

    function drawKmeansChart(showCentroids = true) {
        const baseColors = [
            'rgba(59, 130, 246, 0.5)', 'rgba(239, 68, 68, 0.5)', 'rgba(16, 185, 129, 0.5)',
            'rgba(245, 158, 11, 0.5)', 'rgba(139, 92, 246, 0.5)', 'rgba(236, 72, 153, 0.5)',
            'rgba(52, 211, 153, 0.5)', 'rgba(99, 102, 241, 0.5)'
        ];
        const baseBorderColors = [
            'rgba(59, 130, 246, 1)', 'rgba(239, 68, 68, 1)', 'rgba(16, 185, 129, 1)',
            'rgba(245, 158, 11, 1)', 'rgba(139, 92, 246, 1)', 'rgba(236, 72, 153, 1)',
            'rgba(52, 211, 153, 1)', 'rgba(99, 102, 241, 1)'
        ];
        
        const datasets = kmeansPoints.reduce((acc, point) => {
            const clusterIndex = point.cluster < 0 ? 0 : point.cluster;
            if (!acc[clusterIndex]) {
                 acc[clusterIndex] = {
                    label: `Cluster ${clusterIndex}`,
                    data: [],
                    backgroundColor: point.cluster < 0 ? 'rgba(107, 114, 128, 0.5)' : baseColors[clusterIndex],
                    pointRadius: 5
                };
            }
            acc[clusterIndex].data.push(point);
            return acc;
        }, []);

        if (showCentroids) {
            centroids.forEach((c, i) => {
                datasets.push({
                    label: `Centroid ${i}`,
                    data: [c],
                    backgroundColor: baseBorderColors[i],
                    pointRadius: 8,
                    pointStyle: 'rectRot',
                });
            });
        }

        if (kmeansChart) {
            kmeansChart.data.datasets = datasets.filter(d => d);
            kmeansChart.update('none');
        } else {
            kmeansChart = new Chart(kmeansCanvas, {
                type: 'scatter',
                data: { datasets: datasets.filter(d => d) },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: { min: 0, max: 10 },
                        y: { min: 0, max: 10 }
                    },
                    plugins: { legend: { display: false } }
                }
            });
        }
    }

    async function runKmeans() {
        runKmeansBtn.disabled = true;
        kMeansSlider.disabled = true;
        const k = parseInt(kMeansSlider.value);
        generateKmeansData(k);
        drawKmeansChart();
        await new Promise(r => setTimeout(r, 500));

        for (let iter = 0; iter < 10; iter++) {
            // Assignment step
            kmeansPoints.forEach(p => {
                let minDist = Infinity;
                let bestCluster = -1;
                centroids.forEach((c, i) => {
                    const dist = (p.x - c.x) ** 2 + (p.y - c.y) ** 2;
                    if (dist < minDist) {
                        minDist = dist;
                        bestCluster = i;
                    }
                });
                p.cluster = bestCluster;
            });
            drawKmeansChart();
            await new Promise(r => setTimeout(r, 800));

            // Update step
            const newCentroids = Array(k).fill(0).map(() => ({ x: 0, y: 0, count: 0 }));
            kmeansPoints.forEach(p => {
                newCentroids[p.cluster].x += p.x;
                newCentroids[p.cluster].y += p.y;
                newCentroids[p.cluster].count++;
            });
            
            let moved = false;
            newCentroids.forEach((c, i) => {
                if (c.count > 0) {
                    const newX = c.x / c.count;
                    const newY = c.y / c.count;
                    if(Math.abs(newX - centroids[i].x) > 0.01 || Math.abs(newY - centroids[i].y) > 0.01) {
                        moved = true;
                    }
                    centroids[i] = { x: newX, y: newY };
                }
            });
            drawKmeansChart();
            await new Promise(r => setTimeout(r, 800));
            if (!moved) break;
        }
        runKmeansBtn.disabled = false;
        kMeansSlider.disabled = false;
    }
    runKmeansBtn.onclick = runKmeans;

    // Bias-Variance Chart
    new Chart(document.getElementById('biasVarianceChart'), {
        type: 'line',
        data: {
            labels: ['Low', '', '', '', 'Medium', '', '', '', 'High'],
            datasets: [
                {
                    label: 'Bias',
                    data: [9, 8, 7, 5, 3, 2, 1.5, 1, 0.8],
                    borderColor: 'rgba(59, 130, 246, 1)',
                    borderWidth: 3,
                    tension: 0.4,
                    fill: false,
                },
                {
                    label: 'Variance',
                    data: [0.5, 1, 1.5, 2, 3, 4.5, 6, 8, 9.5],
                    borderColor: 'rgba(239, 68, 68, 1)',
                    borderWidth: 3,
                    tension: 0.4,
                    fill: false,
                },
                {
                    label: 'Total Error',
                    data: [9.5, 9, 8.5, 7, 6, 6.5, 7.5, 9, 10.3],
                    borderColor: 'rgba(16, 185, 129, 1)',
                    borderWidth: 3,
                    borderDash: [5, 5],
                    tension: 0.4,
                    fill: false,
                }
            ]
        },
        options: {
            responsive: true,
            maintainAspectRatio: false,
            scales: {
                x: { title: { display: true, text: 'Model Complexity' } },
                y: { title: { display: true, text: 'Error' }, min: 0 }
            },
            plugins: { legend: { position: 'top' } }
        }
    });
    
    // SVM Chart
    new Chart(document.getElementById('svmChart'), {
        type: 'scatter',
        data: {
            datasets: [
                {
                    label: 'Class 0',
                    data: [{x: 2, y: 3}, {x: 3, y: 4}, {x: 1, y: 5}],
                    backgroundColor: 'rgba(59, 130, 246, 0.5)',
                    pointRadius: 6
                },
                {
                    label: 'Class 1',
                    data: [{x: 6, y: 5}, {x: 7, y: 7}, {x: 8, y: 6}],
                    backgroundColor: 'rgba(239, 68, 68, 0.5)',
                    pointRadius: 6
                },
                {
                    label: 'Hyperplane',
                    data: [{x: 0, y: 1.5}, {x: 10, y: 9.5}],
                    type: 'line',
                    borderColor: 'rgba(124, 58, 237, 1)',
                    borderWidth: 2,
                    fill: false,
                    pointRadius: 0,
                },
                {
                    label: 'Margin',
                    data: [{x: 0, y: 3}, {x: 10, y: 11}],
                    type: 'line',
                    borderColor: 'rgba(167, 139, 250, 0.5)',
                    borderDash: [5, 5],
                    borderWidth: 1,
                    fill: false,
                    pointRadius: 0,
                },
                {
                    label: 'Margin',
                    data: [{x: 1.5, y: 0}, {x: 8.5, y: 7}],
                    type: 'line',
                    borderColor: 'rgba(167, 139, 250, 0.5)',
                    borderDash: [5, 5],
                    borderWidth: 1,
                    fill: false,
                    pointRadius: 0,
                },
                 {
                    label: 'Support Vectors',
                    data: [{x: 3, y: 4}, {x: 6, y: 5}],
                    backgroundColor: 'rgba(255, 255, 255, 0)',
                    borderColor: 'rgba(124, 58, 237, 1)',
                    borderWidth: 2,
                    pointRadius: 8,
                    pointStyle: 'circle'
                }
            ]
        },
        options: { responsive: true, maintainAspectRatio: false, scales: { x: { min: 0, max: 10 }, y: { min: 0, max: 10 } }, plugins: { legend: { display: false } } }
    });


    // Initial draws
    generateKnnData();
    drawKnnChart();
    generateKmeansData(parseInt(kMeansSlider.value));
    drawKmeansChart(false);
});
</script>

</body>
</html>
